{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1: Non-private ERM for Logistic Regression\n",
    "\n",
    "In this assignment, we will build a _logistic regression_ model to perform a classfication task and train it using the _gradient descent_ algorithm. We consider the logistic regression problem in non-private setting. In HW2, we will extend this algorithm to satisfy _differential privacy_ by implementing\n",
    "- Gradient clipping and \n",
    "- Noise injection.\n",
    "\n",
    "## Submission instruction\n",
    "- Due by 11:59pm on **Apr. 10th, 2020**\n",
    "- Complete this notebook and name it as \"*CSCI8960\\_HW1\\_{your first name}.ipynb*\".\n",
    "- Do **not** use any other 3rd party libraries unless you are asked to do so.\n",
    "- Do **not** _hardcode_ any data.\n",
    "- Feel free to modify function prototype (i.e., you can add additional input parameters or remove existing ones based on your needs)\n",
    "- Feel free to define your own functions if needed.\n",
    "- Label your graphs.\n",
    "- All the texts and labels in your graphs should be _legible_.\n",
    "\n",
    "Let's start by importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will be using the telco customer churning dataset. You can dowload the dataset from [here](http://cobweb.cs.uga.edu/~jwlee/datasets/churn.csv).\n",
    "\n",
    "### About the dataset\n",
    "Our goal is to build a logistic regression model for predicting whether the customer will churn or not, i.e., the churn column.\n",
    "\n",
    "| Feature | Description |\n",
    "|:-------:|:----------:|\n",
    "|customerID | Customer ID |\n",
    "| gender | Whether the customer is a male or a female |\n",
    "| SeniorCitizen | Whether the customer is a senior citizen or not (1, 0)|\n",
    "|Partner | Whether the customer has a partner or not (Yes, No) |\n",
    "| Dependents | Whether the customer has dependents or not (Yes, No)|\n",
    "| tenure | Number of months the customer has stayed with the company|\n",
    "|PhoneService | Whether the customer has a phone service or not (Yes, No)|\n",
    "| MultipleLines | Whether the customer has multiple lines or not (Yes, No, No phone service)|\n",
    "| InternetService | Customer’s internet service provider (DSL, Fiber optic, No)|\n",
    "| OnlineSecurity | Whether the customer has online security or not (Yes, No, No internet service) |\n",
    "| OnlineBackup | Whether the customer has online backup or not (Yes, No, No internet service)|\n",
    "| DeviceProtection | Whether the customer has device protection or not (Yes, No, No internet service)|\n",
    "| TechSupport | Whether the customer has tech support or not (Yes, No, No internet service)|\n",
    "| StreamingTV | Whether the customer has streaming TV or not (Yes, No, No internet service)|\n",
    "| StreamingMovies | Whether the customer has streaming movies or not (Yes, No, No internet service) |\n",
    "| Contract | The contract term of the customer (Month-to-month, One year, Two year)|\n",
    "| PaperlessBilling | Whether the customer has paperless billing or not (Yes, No)| \n",
    "| PaymentMethod | The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) | \n",
    "| MonthlyCharges | The amount charged to the customer monthly| \n",
    "| TotalCharges | The total amount charged to the customer| \n",
    "| Churn | Whether the customer churned or not (Yes or No) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Pre-processing the dataset\n",
    "\n",
    "Let's preprocess the dataset for our analysis.\n",
    "\n",
    "### Q1. (23 pts) Load the dataset into a dataframe and performs the one-hot encoding.\n",
    "\n",
    "1. load the dataset into a dataframe. \n",
    "2. If there are any observations with missing values, report their number and remove them.\n",
    "3. `Churn` column contains class labels (two distinct values `Yes` and `No`).\n",
    "4. Use one-hot encoding to convert categorical features into numerical ones.    \n",
    "    - There are multiple way for you to apply the one-hot encoding. Possible choices include:\n",
    "    - Scikit learn's OneHotEncoder class (See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html))\n",
    "    - Padas's `DataFrame` also provides a method: [`pandas.get_dummies()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "5. When you apply the **one-hot encoding**, make sure that you **exclude** the first column (In the example below, this corresponds to the is\\_DSL column).\n",
    "6. For the three numerical variables, scale the features to ensure their values are in [0, 1]. Recall that we discussed the \"feature scaling\" in class.\n",
    "     - The scaling maps the minumn and maximum values of each feature to 0 and 1, respectively.\n",
    "     - You can manually implement this using a datframe's feature (e.g., `df.apply()`).\n",
    "     - You can use `sklearn.preprocessing.MinMaxScaler`. **See** [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) for details.\n",
    "7. Encode your label column (i.e., `Churn`).\n",
    "    - Map \"No\" to 0 and \"Yes\" to 1.\n",
    "    - `sklearn.preprocessing.LabelEncoder` performs this mapping.\n",
    "8. Print out the number of observations and features after the pre-processing.\n",
    "\n",
    "Consider the InternetServics variable in your data. It is a cateorical variable with 3 distinct values: DSL, fiber optic, no. Let's encode the variable using the **one-hot** encoding we used in HW5. This time you can use the function implemented in `pandas` package. See the following example of applying the one-hot encoding to the InternetService variable.\n",
    "\n",
    "| Internet Service |\n",
    "|:------------:|\n",
    "|  DSL         |\n",
    "|  DSL         |\n",
    "|  fiber optic         |\n",
    "|  no         |\n",
    "\n",
    "With one-hot encoding, this will be converted into \n",
    "\n",
    "| is_DSL | is_fiberOptic | is_No |\n",
    "|:------:|:------:|:------:|\n",
    "|   1    |   0    |    0   |\n",
    "|   1    |   0    |    0   |\n",
    "|   0    |   1    |    0   |\n",
    "|   0    |   0    |    1   |.\n",
    "\n",
    "Notice that a categorical variable with 3 distinct values is represented by a binary vector of length 3. In the previous homework, you manually applied the one-hot encoding. In this homework, you are allowed to use either `pandas.get_dummies()` or `sklearn.preprocessing.OneHotEncoder`. **PLEASE**, read the documentation pages ([get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) and [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) for their usages and examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values\n",
      "Int64Index([488, 753, 936, 1082, 1340, 3331, 3826, 4380, 5218, 6670, 6754], dtype='int64')\n",
      "Number of Observations\n",
      "7032\n",
      "Number of Features\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#-------------------------#\n",
    "#  Your code goes here.   #\n",
    "#-------------------------#\n",
    "\n",
    "\n",
    "data = pd.read_csv('churn.csv', header=0)\n",
    "data1 = data.dropna()\n",
    "catkeys = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod']\n",
    "numkeys = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "data1['TotalCharges'] = pd.to_numeric(data1['TotalCharges'], errors='coerce')\n",
    "data2 = data1.dropna()\n",
    "index = data1['TotalCharges'].index[data1['TotalCharges'].apply(np.isnan)]\n",
    "print('Missing values')\n",
    "print(index)\n",
    "\n",
    "lod = []\n",
    "for i in catkeys:\n",
    "    tempdat = data2[i]\n",
    "    encdat = pd.get_dummies(tempdat)\n",
    "    kk = encdat.keys()\n",
    "    lod.append(encdat.drop(kk[0],axis=1))\n",
    "    lod[-1].reset_index(drop=True, inplace=True)\n",
    "    \n",
    "numdat = data2[numkeys]\n",
    "dat1 = numdat.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(dat1)\n",
    "dat1 = scaler.transform(dat1)\n",
    "dat1 = pd.DataFrame(dat1, columns = numkeys)\n",
    "\n",
    "labdat = data2['Churn']\n",
    "le = LabelEncoder()\n",
    "le = le.fit(labdat)\n",
    "dat2 = le.transform(labdat)\n",
    "dat2 = pd.DataFrame(dat2, columns = ['Churn'])\n",
    "\n",
    "lod.append(dat1)\n",
    "lod.append(dat2)\n",
    "findat = pd.concat(lod, axis=1)\n",
    "print('Number of Observations')\n",
    "print(findat.shape[0])\n",
    "print('Number of Features')\n",
    "print(findat.shape[1]-1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we talked in class, we can remove the bias term from the equation by adding a dummy column to the input dataset. In practice, we don't actually need to materialize this dummy column, but in here we will add it to keep the things simple. After adding a dummy column, you dataset will look like:\n",
    "$$ \\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Q2. (2 pts) Augment the dataset by adding a dummy column of ones ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aug_col</th>\n",
       "      <th>Male</th>\n",
       "      <th>1</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Yes</th>\n",
       "      <th>No phone service</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Fiber optic</th>\n",
       "      <th>No</th>\n",
       "      <th>...</th>\n",
       "      <th>One year</th>\n",
       "      <th>Two year</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Credit card (automatic)</th>\n",
       "      <th>Electronic check</th>\n",
       "      <th>Mailed check</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115423</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.215867</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.354229</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.239303</td>\n",
       "      <td>0.210241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.521891</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.662189</td>\n",
       "      <td>0.227521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845274</td>\n",
       "      <td>0.847461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.112935</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.558706</td>\n",
       "      <td>0.033210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.869652</td>\n",
       "      <td>0.787641</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7032 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aug_col  Male  1  Yes  Yes  Yes  No phone service  Yes  Fiber optic  No  \\\n",
       "0         1.0     0  0    1    0    0                 1    0            0   0   \n",
       "1         1.0     1  0    0    0    1                 0    0            0   0   \n",
       "2         1.0     1  0    0    0    1                 0    0            0   0   \n",
       "3         1.0     1  0    0    0    0                 1    0            0   0   \n",
       "4         1.0     0  0    0    0    1                 0    0            1   0   \n",
       "...       ...   ... ..  ...  ...  ...               ...  ...          ...  ..   \n",
       "7027      1.0     1  0    1    1    1                 0    1            0   0   \n",
       "7028      1.0     0  0    1    1    1                 0    1            1   0   \n",
       "7029      1.0     0  0    1    1    0                 1    0            0   0   \n",
       "7030      1.0     1  1    1    0    1                 0    1            1   0   \n",
       "7031      1.0     1  0    0    0    1                 0    0            1   0   \n",
       "\n",
       "      ...  One year  Two year  Yes  Credit card (automatic)  Electronic check  \\\n",
       "0     ...         0         0    1                        0                 1   \n",
       "1     ...         1         0    0                        0                 0   \n",
       "2     ...         0         0    1                        0                 0   \n",
       "3     ...         1         0    0                        0                 0   \n",
       "4     ...         0         0    1                        0                 1   \n",
       "...   ...       ...       ...  ...                      ...               ...   \n",
       "7027  ...         1         0    1                        0                 0   \n",
       "7028  ...         1         0    1                        1                 0   \n",
       "7029  ...         0         0    1                        0                 1   \n",
       "7030  ...         0         0    1                        0                 0   \n",
       "7031  ...         0         1    1                        0                 0   \n",
       "\n",
       "      Mailed check    tenure  MonthlyCharges  TotalCharges  Churn  \n",
       "0                0  0.000000        0.115423      0.001275      0  \n",
       "1                1  0.464789        0.385075      0.215867      0  \n",
       "2                1  0.014085        0.354229      0.010310      1  \n",
       "3                0  0.619718        0.239303      0.210241      0  \n",
       "4                0  0.014085        0.521891      0.015330      1  \n",
       "...            ...       ...             ...           ...    ...  \n",
       "7027             1  0.323944        0.662189      0.227521      0  \n",
       "7028             0  1.000000        0.845274      0.847461      0  \n",
       "7029             0  0.140845        0.112935      0.037809      0  \n",
       "7030             1  0.042254        0.558706      0.033210      1  \n",
       "7031             0  0.915493        0.869652      0.787641      0  \n",
       "\n",
       "[7032 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "#-------------------------#\n",
    "#  Your code goes here.   #\n",
    "#-------------------------#\n",
    "num_rows = len(findat)\n",
    "aug_col = pd.DataFrame(np.ones((num_rows, 1)), columns=['aug_col'])\n",
    "aug_dat = pd.concat([aug_col, findat], axis=1)\n",
    "aug_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-1. (3 pts) Ensure that your dataset has $Y \\in \\{-1, +1 \\}$ .\n",
    "In simplified notation, it is assumed that $y_i \\in \\{-1, +1\\}$. To satisf this assumption, we will **map <font color=\"red\"> \"No\" to -1 and \"Yes\" to +1</font>**.\n",
    "- Write a code that shows your y values indeed has domain of $\\{-1, +1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aug_col</th>\n",
       "      <th>Male</th>\n",
       "      <th>1</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Yes</th>\n",
       "      <th>No phone service</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Fiber optic</th>\n",
       "      <th>No</th>\n",
       "      <th>...</th>\n",
       "      <th>One year</th>\n",
       "      <th>Two year</th>\n",
       "      <th>Yes</th>\n",
       "      <th>Credit card (automatic)</th>\n",
       "      <th>Electronic check</th>\n",
       "      <th>Mailed check</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115423</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.464789</td>\n",
       "      <td>0.385075</td>\n",
       "      <td>0.215867</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.354229</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.239303</td>\n",
       "      <td>0.210241</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.521891</td>\n",
       "      <td>0.015330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.662189</td>\n",
       "      <td>0.227521</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845274</td>\n",
       "      <td>0.847461</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>0.112935</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.558706</td>\n",
       "      <td>0.033210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.869652</td>\n",
       "      <td>0.787641</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7032 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aug_col  Male  1  Yes  Yes  Yes  No phone service  Yes  Fiber optic  No  \\\n",
       "0         1.0     0  0    1    0    0                 1    0            0   0   \n",
       "1         1.0     1  0    0    0    1                 0    0            0   0   \n",
       "2         1.0     1  0    0    0    1                 0    0            0   0   \n",
       "3         1.0     1  0    0    0    0                 1    0            0   0   \n",
       "4         1.0     0  0    0    0    1                 0    0            1   0   \n",
       "...       ...   ... ..  ...  ...  ...               ...  ...          ...  ..   \n",
       "7027      1.0     1  0    1    1    1                 0    1            0   0   \n",
       "7028      1.0     0  0    1    1    1                 0    1            1   0   \n",
       "7029      1.0     0  0    1    1    0                 1    0            0   0   \n",
       "7030      1.0     1  1    1    0    1                 0    1            1   0   \n",
       "7031      1.0     1  0    0    0    1                 0    0            1   0   \n",
       "\n",
       "      ...  One year  Two year  Yes  Credit card (automatic)  Electronic check  \\\n",
       "0     ...         0         0    1                        0                 1   \n",
       "1     ...         1         0    0                        0                 0   \n",
       "2     ...         0         0    1                        0                 0   \n",
       "3     ...         1         0    0                        0                 0   \n",
       "4     ...         0         0    1                        0                 1   \n",
       "...   ...       ...       ...  ...                      ...               ...   \n",
       "7027  ...         1         0    1                        0                 0   \n",
       "7028  ...         1         0    1                        1                 0   \n",
       "7029  ...         0         0    1                        0                 1   \n",
       "7030  ...         0         0    1                        0                 0   \n",
       "7031  ...         0         1    1                        0                 0   \n",
       "\n",
       "      Mailed check    tenure  MonthlyCharges  TotalCharges  Churn  \n",
       "0                0  0.000000        0.115423      0.001275     -1  \n",
       "1                1  0.464789        0.385075      0.215867     -1  \n",
       "2                1  0.014085        0.354229      0.010310      1  \n",
       "3                0  0.619718        0.239303      0.210241     -1  \n",
       "4                0  0.014085        0.521891      0.015330      1  \n",
       "...            ...       ...             ...           ...    ...  \n",
       "7027             1  0.323944        0.662189      0.227521     -1  \n",
       "7028             0  1.000000        0.845274      0.847461     -1  \n",
       "7029             0  0.140845        0.112935      0.037809     -1  \n",
       "7030             1  0.042254        0.558706      0.033210      1  \n",
       "7031             0  0.915493        0.869652      0.787641     -1  \n",
       "\n",
       "[7032 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \n",
    "    #-------------------------#\n",
    "    #  Your code goes here.   #\n",
    "    #-------------------------#\n",
    "    \n",
    "    aug_dat['Churn'] = aug_dat['Churn'].replace(0,-1)\n",
    "    aug_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3-2 (2pts) Split the data into train and test sets.\n",
    "- Use `sklearn.model_selection.train_test_split()` function\n",
    "- Use 80% of observations for training and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-------------------------#\n",
    "#  Your code goes here.   #\n",
    "#-------------------------#\n",
    "\n",
    "y = aug_dat['Churn']\n",
    "X = aug_dat.drop(columns=['Churn'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframes to numpy arrays for later calculation\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Logistic Regression Model\n",
    "\n",
    "Now we are ready to build our classfication model. Recall that logistic regression model tries to learn the conditional probability using a sigmoid function:\n",
    "$$ \\Pr[Y = y_i~|~X=x_i] = \\sigma(y_i\\beta^\\intercal x_i) \\quad \\text{ for }i=1, \\ldots, n\\,, $$\n",
    "where $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)^\\intercal$.\n",
    "\n",
    "Recall the **empirical risk minimization** framework we discussed in class in which our goal for _logistic regression_ is to minimize the following *empirical logistic loss*:\n",
    "$$ \\underset{\\mathbf{\\beta} \\in \\mathbb{R}^{p+1}}{\\text{minimize}}\\, \\mathbb{E}[\\ell(\\beta)] \\approx \\frac{1}{n}\\sum_{i=1}^n \\log (1 + \\exp(-y_i\\mathbf{\\beta}^\\intercal\\mathbf{x}_i))\\,, $$\n",
    "and its gradient is given by\n",
    "$$ \\frac{\\partial \\ell(\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} = \\frac{1}{n}\\sum_{i=1}^n (1 - \\sigma(y_i\\mathbf{\\beta}^\\intercal\\mathbf{x}_i))y_i\\mathbf{x}_i\\,. \\qquad\\qquad (1) $$\n",
    "\n",
    "I strongly recommend you to derive and verify the above equation for the gradient by yourself. One simple but powerful method you can use to solve the above optimization problem is **<font color=\"red\">gradient descent</font>** algorithm. Starting from an inital solution $\\beta^{(0)}$, it iteratively computes the *gradient* at the given location $\\beta^{(k)}$ and updates the solution as follows:\n",
    "$$\\beta^{(k+1)} = \\beta^{(k)} - \\eta_k \\nabla \\ell(\\beta^{(k)})\\,,$$\n",
    "where\n",
    "- $\\beta^{(k)}$ denotes the parameter vector at iteration $k$,\n",
    "- $\\eta_k > 0$ is the step size for iteration $k$, and \n",
    "- $\\nabla \\ell(\\beta^{(k)})$ is the gradient of the objective function at $\\beta^{(k)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. (10 pts) Implement the objective function of logistic regression model.\n",
    "- You will get the full mark on this question if you implement this without using a for-loop.\n",
    "\n",
    "Given a numpy array $\\beta$, the `logres_obj()` function should return \n",
    "$$ \\frac{1}{n}\\sum_{i=1}^n \\log(1 + \\exp(-y_i\\beta^\\intercal x_i))\\,.$$\n",
    "\n",
    "**<font color=\"red\">Hint</font>**: let \n",
    "$$ W = \\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33} \\\\\n",
    "w_{41} & w_{42} & w_{43} \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{4\\times 3}\\,,\\quad q = \\begin{pmatrix}\n",
    "q_1 \\\\ q_2 \\\\ q_3 \\\\ q_4 \\end{pmatrix} \\in \\mathbb{R}^{4\\times 1}\\,, \\quad \\text{ and } \\alpha = \n",
    "\\begin{pmatrix}\n",
    "\\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 1}.$$\n",
    "Then, we can compute $\\sum_{i=1}^4 \\alpha^\\intercal w_i = \\sum_{i=1}^4 w_i^\\intercal \\alpha$ as follows:\n",
    "$$\n",
    "\\sum_{i=1}^4 \\alpha^\\intercal w_i \n",
    "= \\text{np.sum}\\left\\{W \\alpha\\right\\}\n",
    "= \\text{np.sum}\\left\\{\n",
    "\\begin{pmatrix}\n",
    "\\alpha^\\intercal w_1 \\\\\n",
    "\\alpha^\\intercal w_2 \\\\\n",
    "\\alpha^\\intercal w_3 \\\\\n",
    "\\alpha^\\intercal w_4 \n",
    "\\end{pmatrix} \\right\\}\\,,\n",
    "$$\n",
    "where $w_i = (w_{i1}, w_{i2}, w_{i3})$, i.e, the $i^\\mathrm{th}$ row. See the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "alpha = np.array([1, 2, 3])\n",
    "q = np.array([1, 2, 3, 4])\n",
    "\n",
    "sum_wa = np.sum(np.dot(W, alpha), axis=0)\n",
    "print(sum_wa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute $\\sum_{i=1}^4 \\log(\\alpha^\\intercal w_i)$, you can do \n",
    "$$\n",
    "\\sum_{i=1}^4 \\log\\alpha^\\intercal w_i \n",
    "= \\text{np.sum}\\left\\{\\text{np.log}(W \\alpha)\\right\\}\n",
    "= \\text{np.sum}\\left\\{\n",
    "\\begin{pmatrix}\n",
    "\\log \\alpha^\\intercal w_1 \\\\\n",
    "\\log\\alpha^\\intercal w_2 \\\\\n",
    "\\log\\alpha^\\intercal w_3 \\\\\n",
    "\\log\\alpha^\\intercal w_4 \n",
    "\\end{pmatrix}, axis=0 \\right\\}\\,,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.236323943019237\n"
     ]
    }
   ],
   "source": [
    "log_wa = np.log(np.dot(W, alpha))\n",
    "print(np.sum(log_wa, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{i=1}^4 w_i q_i = \\text{np.sum}\\left\\{ \n",
    "\\begin{pmatrix}\n",
    "q_1 \\cdot w_1 \\\\\n",
    "q_2 \\cdot w_2 \\\\\n",
    "q_3 \\cdot w_3 \\\\\n",
    "q_4 \\cdot w_4 \n",
    "\\end{pmatrix}, axis=0\\right\\}\\,.$$ Refer to the lecture slides on <font color=\"red\">numpy array broadcasting</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.shape= (4,)\n",
      "q2d.shape= (1, 4)\n",
      "(after transpose) q2d.shape= (4, 1)\n",
      "[[ 1  2  3]\n",
      " [ 8 10 12]\n",
      " [21 24 27]\n",
      " [40 44 48]]\n"
     ]
    }
   ],
   "source": [
    "print(\"q.shape=\", q.shape)\n",
    "q2d = np.atleast_2d(q)\n",
    "print(\"q2d.shape=\", q2d.shape)\n",
    "q2d = q2d.T\n",
    "print(\"(after transpose) q2d.shape=\", q2d.shape)\n",
    "print(W * q2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454\n",
      "8.429533722725607\n"
     ]
    }
   ],
   "source": [
    "def logres_obj(X, y, beta):\n",
    "    \"\"\"\n",
    "    returns the objective value of the logistic regression model\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    X, y: numpy arrays representing the input dataset\n",
    "    beta: numpy array correspoinding to the parameter vector\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    obj: objective value (scalar)\n",
    "    \"\"\"\n",
    "    #-------------------------#\n",
    "    #  Your code goes here.   #\n",
    "    #-------------------------#\n",
    "    lst = [-y[i] * np.dot(beta, X[i]) for i in range(0,len(y))]\n",
    "    obj = np.sum(np.log(1+np.exp(lst)))/len(y) \n",
    "    \n",
    "    return obj\n",
    "\n",
    "\n",
    "# test code assuming (train_X, train_y) represents your training dataset\n",
    "beta = np.zeros(X_train.shape[1])\n",
    "print(logres_obj(X_train, y_train, beta))\n",
    "\n",
    "beta = np.ones(X_train.shape[1])\n",
    "print(logres_obj(X_train, y_train, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. (10 pts) Implement the function that returns the gradient of logistic regression model.\n",
    "The `logres_grad()` function should return\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n (\\sigma(y_i\\mathbf{\\beta}^\\intercal\\mathbf{x}_i)-1)y_i\\mathbf{x}_i \\,.$$\n",
    "\n",
    "Note that the output is a gradient (1D numpy array of length $p+1$). \n",
    "- You should carefully distinguish vectors from scalars.\n",
    "- Use the python's implementation of simoid function, `scipy.special.expit()` (check its documentation [here](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.special.expit.html))\n",
    "- To help your implementation, a function you can use to numerically approximate the gradient is provided. See `finite_diff_grad()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff_grad(X, y, beta, h=0.00001):\n",
    "    \"\"\" \n",
    "    This function numerically evaluates the gradient of the objective function at beta\n",
    "    \"\"\" \n",
    "    \n",
    "    f_beta = logres_obj(X, y, beta)\n",
    "    \n",
    "    p = len(beta)\n",
    "    grad = np.zeros(p)\n",
    "    \n",
    "    for i in range(p):\n",
    "        e = np.zeros_like(beta)\n",
    "        e[i] = h\n",
    "        \n",
    "        # evalute f(x+h)\n",
    "        f_beta_h1 = logres_obj(X, y, beta+e)\n",
    "        \n",
    "        # evaluate f(x - h)\n",
    "        f_beta_h2 = logres_obj(X, y, beta-e)\n",
    "        \n",
    "        grad[i] = (f_beta_h1 - f_beta_h2) / (2.0 * h)\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def logres_grad(X, y, beta):\n",
    "    \"\"\"\n",
    "    returns the gradient of logistic regression model\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    X, y: numpy arrays representing the input dataset\n",
    "    beta: numpy array correspoinding to the parameter vector\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    grad: 1D numpy array\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    #         Your code goes here        #\n",
    "    ######################################\n",
    "    lst = np.array([(expit(y[i] * np.dot(beta, X[i]))-1)*y[i]*X[i] for i in range(0,len(y))])\n",
    "    grad = np.average(lst, axis=0)\n",
    "    \n",
    "    return grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 distance= 2.938101848733794e-10\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "\"\"\"\n",
    "The outputs of two functions, logres_grad() and finite_diff_grad(), should be \n",
    "approximately the same.\n",
    "\"\"\"\n",
    "grad_analytic = logres_grad(X, y, beta)\n",
    "grad_numeric = finite_diff_grad(X, y, beta)\n",
    "\n",
    "print(\"L2 distance=\", np.linalg.norm(grad_analytic - grad_numeric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. (10 pts) Implment the Gradient Descent Algorithm\n",
    "Using the two functions `logres_obj()` and `logres_grad()`, implement the gradient descent algorithm. The core of gradient descent algorithm is $$\\beta^{(k+1)}=\\beta^{(k)}-\\eta\\nabla f(\\beta^{(k)})$$\n",
    "\n",
    "1. Initialize the $\\beta^{(0)} = (0, 0, \\ldots, 0)^\\intercal$.\n",
    "2. Use the fixed step size $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(X, y, n_iter=100):\n",
    "    \"\"\"\n",
    "    gradient descent algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    X, y: input dataset\n",
    "    n_iter: the number of iterations\n",
    "                 \n",
    "    Returns:\n",
    "    -----------\n",
    "    sol_path : a list of solutions, the kth entry corresponds to the beta \n",
    "               at iteration k\n",
    "    obj_vals : a list of object values, the kth entry corresponds to the \n",
    "               objective value at iteration k               \n",
    "    \"\"\"\n",
    "    sol_path = []\n",
    "    obj_vals = []\n",
    "    eta = 1.0    \n",
    "    \n",
    "    n, p = X.shape\n",
    "    \n",
    "    # initialize your parameter vector, beta, here\n",
    "    beta = np.zeros(p)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        #---------------------------#\n",
    "        #    Your code goes here    #\n",
    "        #---------------------------#\n",
    "        sol_path.append(beta) \n",
    "        obj_vals.append(logres_obj(X,y,beta))\n",
    "        beta = beta - eta * logres_grad(X,y,beta)\n",
    "        \n",
    "    return sol_path, obj_vals \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Recall that in logistic regression we set \n",
    "$$ \\Pr(Y=1~|~X=x) = \\sigma(\\mathbf{\\beta}^\\intercal\\mathbf{x}). $$\n",
    "Our prediction will be\n",
    "$$\n",
    "Y = \\begin{cases}\n",
    "1 & \\mbox{ if $\\Pr[Y=1~|~X=x] \\geq 0.5$} \\\\\n",
    "-1 & \\mbox{ otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "Since the sigmoid function $\\sigma(x)$ is greater than or equal to 0.5 when $x\\geq 0$ (refer to the figure below), we will predict $Y=1$ when $\\mathrm{sign}(\\beta^\\intercal x)$ is positive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b95be27fc8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = expit(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, 'r-', label='sigmoid')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(r'$\\sigma(x)$')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. (10 pts) Implement a function that predicts the labels for the given examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, beta):\n",
    "    \"\"\"\n",
    "    predict the labels of observations in X\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------\n",
    "    beta: coefficients\n",
    "    X: 2D numpy array, a set of observations, an example per row\n",
    "    \n",
    "    Returns:\n",
    "    y_hat: predicted labels    \n",
    "    \"\"\"\n",
    "    \n",
    "    #---------------------------#\n",
    "    #    Your code goes here    #\n",
    "    #---------------------------#\n",
    "    lst = [expit(np.dot(beta, X[i])) >= 0.5 for i in range(0,X.shape[0])]\n",
    "    y_hat = [1 if x==True else -1 for x in lst]\n",
    "\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Training\n",
    "Now we're ready to train the logistic regression model using our implementation of gradient descent. Let's train a logistic regression model using the functions you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 500\n",
    "sols, objs = grad_desc(X_train, y_train, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 500 elements in the array `sols`, and each element is an estimate of $\\beta$ (or solution) at iteration $k$. We can expect that the accuracy of $\\beta$ improves as $k$ increases.\n",
    "\n",
    "### Q8. (10 pts) Draw a plot showing how the objective values change as we do more gradient descent updates.\n",
    "In other words, x-axis = iteration number $k$ and y-axis = objective value at iteration $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b95bf46808>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXwV9b3/8deHEIzIIptXFDCIwSUQAgYE0YIiyK91u4qiUi/UVtTWrba2cL2iore16q22/dne4r2IVqsILo2tVqkL1rolUdzAhUUkQmXfZAvwuX/M5HByziQcQk5Olvfz8ZjHOfOd78x85hDO53y/M/Mdc3dEREQStch0ACIi0jApQYiISCQlCBERiaQEISIikZQgREQkUstMB1BXOnfu7Lm5uZkOQ0SkUSkrK1vt7l2iljWZBJGbm0tpaWmmwxARaVTMbGl1y9LaxWRmo83sEzNbaGaTIpbfY2bzwulTM1sft2y8mX0WTuPTGaeIiCRLWwvCzLKA+4CRQDlQYmbF7j6/so67/zCu/tVA//B9R+BmoAhwoCxcd1264hURkarS2YIYBCx098XuvgN4DDi7hvoXAY+G708H5rj72jApzAFGpzFWERFJkM5zEIcDy+Lmy4EToiqa2RFAT+ClGtY9PGK9icBEgB49eux/xNIkVFRUUF5ezrZt2zIdikiDkZOTQ7du3cjOzk55nXQmCIsoq27gpwuB2e6+a1/WdfdpwDSAoqIiDSolAJSXl9O2bVtyc3Mxi/pTEmle3J01a9ZQXl5Oz549U14vnV1M5UD3uPluwPJq6l7Inu6lfV1XpIpt27bRqVMnJQeRkJnRqVOnfW5VpzNBlAB5ZtbTzFoRJIHixEpmdjTQAXgjrvh5YJSZdTCzDsCosEwkJUoOIlXV5v9E2hKEu+8EriL4Yl8APO7uH5nZVDM7K67qRcBjHjfuuLuvBW4jSDIlwNSwrO5t3gw33QRvvZWWzYuINFZpvQ/C3Z91997u3svd/zMsm+LuxXF1bnH3pHsk3H26ux8VTg+kLcitW+H220E32UkdKi8v5+yzzyYvL49evXpx7bXXsmPHDgBmzJjBVVddFbneiSeeWKv9Pf3008yfH7uCnClTpvC3v/2tVtvaHxMmTGD27Nlp2/769ev57W9/m7btQ83/PvG+973vxT7zn/3sZ3Uew/Lle3rV4/dVnzQWU2Wza/fuzMYhTYa7c+6553LOOefw2Wef8emnn7J582ZuvPHGva77+uuv12qfiQli6tSpnHbaabXaVkNWHwkiVf/zP//DcccdB9QuQezatavaZYkJIn5f9UkJojJB6Ml6UkdeeuklcnJy+M53vgNAVlYW99xzD9OnT2fLli0ALFu2jNGjR3P00Udz6623xtZt06ZN7P1dd93FwIEDKSgo4Oabb46VP/TQQxQUFNCvXz8uueQSXn/9dYqLi7nhhhsoLCxk0aJFsV/yzz33HBdccEFs3VdeeYUzzzwTgBdeeIEhQ4YwYMAAzj//fDZv3lzlOBYsWMCgQYNi859//jkFBQVAkIAGDhxInz59mDhxIlFPpszNzWX16tUAlJaWMnz4cAC+/vprLr30UgYOHEj//v3505/+lLTu5s2bGTFiBAMGDKBv376xOpMmTWLRokUUFhZyww03JK338MMPM2jQIAoLC7n88stjX8JXXnklRUVF5OfnV/ksS0pKOPHEE+nXrx+DBg1i06ZNACxfvpzRo0eTl5fHT37yk6T9AAwfPpzS0lImTZrE1q1bKSwsZNy4cTXG0aZNG6ZMmcIJJ5zAG2+8Efk5zp49m9LSUsaNG0dhYSFbt26N7Qvg0UcfpW/fvvTp04ef/vSnsXjatGnDjTfeSL9+/Rg8eDBfffVVZNz7xN2bxHT88cd7raxd6w7u995bu/WlwZk/f/6emWuvdR82rG6na6+tcf+/+tWv/LrrrksqLyws9Pfee88feOABP/TQQ3316tW+ZcsWz8/P95KSEnd3P+igg9zd/fnnn/fLLrvMd+/e7bt27fJvfetbPnfuXP/www+9d+/evmrVKnd3X7Nmjbu7jx8/3mfNmhXbV+V8RUWFd+/e3Tdv3uzu7ldccYX/4Q9/8FWrVvnJJ58cK7/jjjv81ltvTYq5X79+vmjRolid2267rcp+3d2//e1ve3FxcVIcRxxxRCzOkpISHzZsmLu7T5482f/whz+4u/u6des8Ly8vFkeliooK37Bhg7u7r1q1ynv16uW7d+/2JUuWeH5+fuTnPn/+fD/jjDN8x44d7u5+5ZVX+oMPPlgl3p07d/qwYcP8vffe8+3bt3vPnj397bffdnf3DRs2eEVFhT/wwAPes2dPX79+vW/dutV79OjhX3zxRdL+hg0blvTvtrc4AJ85c2asbnWfY/y24+e//PJL7969u69cudIrKir8lFNO8aeeeiq27cr1b7jhhti/VeJnlAgo9Wq+V5vMYH21pi4mqWPuHnnFSHz5yJEj6dSpEwDnnnsur732GkVFRbG6L7zwAi+88AL9+/cHgl/Un332Ge+99x5jxoyhc+fOAHTs2LHGWFq2bMno0aN55plnGDNmDH/5y1+48847mTt3LvPnz2fo0KEA7NixgyFDhiStf8EFF/D4448zadIkZs6cycyZMwF4+eWXufPOO9myZQtr164lPz8/1jLZmxdeeIHi4mLuvvtuILgs+YsvvuDYY4+t8ln9+7//O6+++iotWrTgyy+/3Osv4hdffJGysjIGDhwIwNatWznkkEMAePzxx5k2bRo7d+5kxYoVzJ8/HzOja9eusfrt2rWLbWvEiBG0b98egOOOO46lS5fSvXt3UlFTHFlZWZx33nmxuvv6OZaUlDB8+HC6dAkGXx03bhyvvvoq55xzDq1ateKMM84A4Pjjj2fOnDkpxVsTJYgWYS+bupiapnvvrfdd5ufn88QTT1Qp27hxI8uWLaNXr16UlZUlJZDEeXdn8uTJXH755VXKf/3rX+/z5Ypjx47lvvvuo2PHjgwcOJC2bdvi7owcOZJHH310r+uef/75nHvuuZgZeXl5bNu2je9///uUlpbSvXt3brnllsjr61u2bMnu8IdX/HJ354knnuDoo4+udr+PPPIIq1atoqysjOzsbHJzc/d6Db+7M378eH7+859XKV+yZAl33303JSUldOjQgQkTJrBt27ZqEznAAQccEHuflZXFzp07a9x3KnFAcDdzVlYWQMqfY+K2q5OdnR07nn2NuTo6B6EWhNSxESNGsGXLFh566CEgOBn5ox/9iAkTJtC6dWsA5syZw9q1a9m6dStPP/107Jd8pdNPP53p06fHzgt8+eWXrFy5khEjRvD444+zZs0aANauDa7+btu2baz/PNHw4cN55513uP/++xk7diwAgwcP5h//+AcLFy4EYMuWLXz66adJ6/bq1YusrCxuu+222LqVX2KdO3dm8+bN1V61lJubS1lZGUCVhHn66afzm9/8JvZl9+677yatu2HDBg455BCys7N5+eWXWbp06V6Pc8SIEcyePZuVK1fGPpulS5eyceNGDjroINq3b89XX33Fc889B8AxxxzD8uXLKSkpAWDTpk21/lLNzs6moqKixjgS1fQ5VnecJ5xwAnPnzmX16tXs2rWLRx99lGHDhtUq5lQoQegktdQxM+Opp55i1qxZ5OXl0bt3b3Jycqpc6XLSSSdxySWXUFhYyHnnnRfrXqr8BThq1CguvvhihgwZQt++fRkzZgybNm0iPz+fG2+8kWHDhtGvXz+uv/56AC688ELuuusu+vfvz6JFi6rEk5WVxRlnnMFzzz0X64Lo0qULM2bM4KKLLqKgoIDBgwfz8ccfRx7P2LFjefjhh2Mnuw8++GAuu+wy+vbtyznnnBPrSkl08803c+2113LyySfHfjUD3HTTTVRUVFBQUECfPn246aabktYdN24cpaWlFBUV8cgjj3DMMccA0KlTJ4YOHUqfPn2STlIfd9xx3H777YwaNYqCggJGjhzJihUr6NevH/379yc/P59LL700loxbtWrFzJkzufrqq+nXrx8jR46s9fhdEydOpKCggHHjxlUbR6KaPscJEyZwxRVXxE5SV+ratSs///nPOeWUU+jXrx8DBgzg7LNrGgN1/1hNTZbGpKioyGv1wKAtW+Cgg+AXv4BqrlaQxmXBggVV+rMbizVr1jBgwIDIX5sidSHq/4aZlbl7UVR9tSDUxSQNwPLlyxkyZAg//vGPMx2KSIxOUquLSRqAww47LPIcgEgmqQWhq5iapKbSdSpSV2rzf0IJQl1MTU5OTg5r1qxRkhAJefg8iJycnH1aT11MakE0Od26daO8vJxVq1ZlOhSRBqPyiXL7QglCLYgmJzs7e5+emiUi0dTFpJPUIiKRlCCUIEREIilBQJAk1MUkIlKFEgQEJ6rVghARqUIJAtSCEBGJoAQBQYJQC0JEpAolCFAXk4hIBCUIUBeTiEgEJQhQC0JEJIISBKgFISISQQkCdJJaRCSCEgSoi0lEJIISBKiLSUQkQloThJmNNrNPzGyhmU2qps4FZjbfzD4ysz/Gle8ys3nhVJzOONXFJCKSLG3DfZtZFnAfMBIoB0rMrNjd58fVyQMmA0PdfZ2ZHRK3ia3uXpiu+Kpo0UItCBGRBOlsQQwCFrr7YnffATwGnJ1Q5zLgPndfB+DuK9MYT/XUghARSZLOBHE4sCxuvjwsi9cb6G1m/zCzN81sdNyyHDMrDcvPidqBmU0M65Tu19PDdJJaRCRJOp8oZxFlid/CLYE8YDjQDfi7mfVx9/VAD3dfbmZHAi+Z2QfuvqjKxtynAdMAioqKav8Nr5PUIiJJ0tmCKAe6x813A5ZH1PmTu1e4+xLgE4KEgbsvD18XA68A/dMWqbqYRESSpDNBlAB5ZtbTzFoBFwKJVyM9DZwCYGadCbqcFptZBzM7IK58KDCfdFEXk4hIkrR1Mbn7TjO7CngeyAKmu/tHZjYVKHX34nDZKDObD+wCbnD3NWZ2IvB7M9tNkMTuiL/6qc6pi0lEJEk6z0Hg7s8CzyaUTYl778D14RRf53Wgbzpjq0ItCBGRJLqTGtSCEBGJoAQBOkktIhJBCQLUxSQiEkEJAtTFJCISQQkC1IIQEYmgBAFqQYiIRFCCAJ2kFhGJoAQB6mISEYmgBAHqYhIRiaAEAepiEhGJoAQBeqKciEgEJQhQC0JEJIISBOgktYhIBCUI0ElqEZEIShCgLiYRkQhKEKAuJhGRCEoQoC4mEZEIShCgFoSISAQlCFALQkQkghIE6CS1iEgEJQhQF5OISAQlCFAXk4hIBCUIUAtCRCSCEgSoBSEiEiGlBGFmB5rZ0ekOJmN0klpEJMleE4SZnQnMA/4azheaWXG6A6tX6mISEUmSSgviFmAQsB7A3ecBuekLKQPUxSQikiSVBLHT3TfUZuNmNtrMPjGzhWY2qZo6F5jZfDP7yMz+GFc+3sw+C6fxtdl/ytSCEBFJ0jKFOh+a2cVAlpnlAdcAr+9tJTPLAu4DRgLlQImZFbv7/Lg6ecBkYKi7rzOzQ8LyjsDNQBHgQFm47rp9O7wUqQUhIpIklRbE1UA+sB14FNgIXJfCeoOAhe6+2N13AI8BZyfUuQy4r/KL391XhuWnA3PcfW24bA4wOoV91o5OUouIJNlrC8LdtwA3htO+OBxYFjdfDpyQUKc3gJn9A8gCbnH3v1az7uGJOzCzicBEgB49euxjeHHUxSQikmSvCcLMXibo5qnC3U/d26oRZYnbaQnkAcOBbsDfzaxPiuvi7tOAaQBFRUW1/4ZXF5OISJJUzkH8OO59DnAesDOF9cqB7nHz3YDlEXXedPcKYImZfUKQMMoJkkb8uq+ksM/aUReTiEiSVLqYyhKK/mFmc1PYdgmQZ2Y9gS+BC4GLE+o8DVwEzDCzzgRdTouBRcDPzKxDWG8Uwcns9FAXk4hIklS6mDrGzbYAjgcO3dt67r7TzK4Cnic4vzDd3T8ys6lAqbsXh8tGmdl8YBdwg7uvCfd7G0GSAZjq7mv34bj2jbqYRESSpNLFVEbQ/28EXUtLgO+msnF3fxZ4NqFsStx7B64Pp8R1pwPTU9nPflMLQkQkSSpdTD3rI5CMUgtCRCRJtQnCzM6taUV3f7Luw8kQnaQWEUlSUwvizBqWOdB0EoS6mEREklSbINz9O/UZSEapi0lEJEkqJ6kxs28RDLeRU1nm7lPTFVS9UwtCRCRJKs+D+G9gLMGYTAacDxyR5rjql1oQIiJJUhms70R3/zdgnbvfCgyh6h3SjZ9OUouIJEklQWwNX7eY2WFABdC0Ln1VF5OISJJUzkH82cwOBu4C3iG4gun+tEZV39TFJCKSpKb7ILLdvcLdbwuLnjCzPwM5tX3CXIOlFoSISJKaupi+NLP7zexUMzMAd9/e5JIDqAUhIhKhpgRxLFAK3AQsM7N7zSzxgT9Ng05Si4gkqTZBuPsad/+9u59C8PjQJcC9ZrbIzP6z3iKsD+piEhFJkspVTLj7cuB/gd8Bm4DvpTOoeqcuJhGRJDUmCDPLMbPzzexJgof4jCB4cM9h9RFcvVEXk4hIkpquYvojcBrwKvBH4GJ331ZfgdUrdTGJiCSp6T6I54HL3X1TfQWTMepiEhFJUtNorg/WZyAZpRaEiEiSlE5SN3lqQYiIJFGCAJ2kFhGJkMpw363N7CYzuz+czzOzM9IfWj1SF5OISJJUWhAPANsJhvkGKAduT1tEmaAuJhGRJKkkiF7ufifBMN+4+1aCBwc1HWpBiIgkSSVB7DCzAwmG+cbMehG0KJoOtSBERJKk8jyIW4C/At3N7BFgKDAhjTHVP52kFhFJstcE4e4vmFkZMJiga+lad1+d9sjqk7qYRESS7DVBmFkx8ChQ7O5fpz+kDFAXk4hIklTOQfwXcDIw38xmmdkYM8tJZeNmNtrMPjGzhWY2KWL5BDNbZWbzwul7cct2xZUXp3xEtaEWhIhIklS6mOYCc80sCzgVuAyYDrSrab2w/n3ASIJLY0vMrNjd5ydUnenuV0VsYqu7F6ZwDPtPLQgRkSQp3UkdXsV0HnAFMBBIZZymQcBCd1/s7juAx4CzaxtoWukktYhIklTupJ4JLCBoPdxHcF/E1Sls+3BgWdx8eViW6Dwze9/MZptZ97jyHDMrNbM3zeycamKbGNYpXbVqVQohVUNdTCIiSVK9k7qXu1/h7i+5e6p9MVE30yV+Cz8D5Lp7AfA3qrZMerh7EXAxwaNOeyVtzH2auxe5e1GXLl1SDCsqUnUxiYgkqumBQae6+0tAa+Bss6rf9+7+5F62XQ7Etwi6AcsTtrEmbvZ+4Bdxy5aHr4vN7BWgP8FT7eqeuphERJLUdJJ6GPAScGbEMgf2liBKgDwz6wl8CVxI0BqIMbOu7r4inD2LoCsLM+sAbHH37WbWmeDmvDv3sr/aa9FCLQgRkQQ1PTDo5vDtVHdfEr8s/NKvkbvvNLOrCJ5MlwVMd/ePzGwqUOruxcA1ZnYWsBNYy547tI8Ffm9muwm6we6IuPqp7ljTGlpKRKQupDLUxhPAgISy2cDxe1vR3Z8Fnk0omxL3fjIwOWK914G+KcRWN1q0qNyxkoWISKimcxDHAPlAezM7N25ROyClG+UajcqksHs3ZGVlNhYRkQaiphbE0cAZwMFUPQ+xieBmuaajMkHoRLWISExN5yD+BPzJzIa4+xv1GFP9i+9iEhERILX7IK4ws4MrZ8ysg5lNT2NM9a8yQezaldk4REQakFQSRIG7r6+ccfd1BPckNB054SmVbdsyG4eISAOSSoJoEd6XAICZdSS1q58ajwMPDF6VIEREYlL5ov8v4HUzm01wg9wFwH+mNar6VtmC2Lo1s3GIiDQgqQz3/ZCZlRIM1mfAuWm9aS0TKlsQShAiIjEpDfcNdAS+dvffAKtSuZO6UVEXk4hIklSG+74Z+Cl77njOBh5OZ1D1Tl1MIiJJUmlB/CvBQHpfQ2yU1bbpDKreqYtJRCRJKglih7s74bMczOyg9IaUAepiEhFJkkqCeNzMfg8cbGaXETzY5/70hlXP1MUkIpIklauY7jazkcBGgvGZprj7nLRHVp/UxSQikiSlG97ChNC0kkI8dTGJiCSptovJzF4LXzeZ2caIaYmZfb/+Qk0jdTGJiCSpaTTXk8LXyCuWzKwT8Drw2/SEVo/UxSQikiSlLiYzGwCcRHAl02vu/q67rzGz4ekMrt5osD4RkSSp3Cg3BXgQ6AR0BmaY2X8AuPuK9IZXT8zggAPUghARiZNKC+IioL+7bwMwszuAd4Db0xlYvTvwQLUgRETipHIfxOdUfQb1AcCitESTSQceqBaEiEicalsQZvYbgnMO24GPzGxOOD8SeK1+wqtHBx4IW7ZkOgoRkQajpi6m0vC1DHgqrvyVtEWTSe3awaZNmY5CRKTBqOky1wcBzCwHOIqg9bCo8lxEk9OuHWzcmOkoREQajJpulGtpZncC5QRXMT0MLDOzO80su74CrDft2sGGDZmOQkSkwajpJPVdBA8K6unux7t7f6AXcDBwd30EV6/at1cLQkQkTk0J4gzgMnePdcy7+0bgSuCbqWzczEab2SdmttDMJkUsn2Bmq8xsXjh9L27ZeDP7LJzGp35ItaQuJhGRKmo6Se3hcyASC3eZWVJ5IjPLAu4juOqpHCgxs+KI51nPdPerEtbtCNwMFBGc+ygL1123t/3WmrqYRESqqKkFMd/M/i2x0My+DXycwrYHAQvdfbG77wAeA85OMa7TgTnuvjZMCnOA0SmuWzvt28OOHbB9e1p3IyLSWNTUgvgB8KSZXUpwqasDA4EDCR5DujeHA8vi5suBEyLqnWdm3wA+BX7o7suqWffwxBXNbCIwEaBHjx4phFSDdu2C1w0b4JBD9m9bIiJNQLUtCHf/0t1PAKYS3E39BTDV3Qe5+5cpbNuiNpsw/wyQ6+4FBE+qe3Af1sXdp7l7kbsXdenSJYWQalCZIHQeQkQESO2Jci8BL9Vi2+VA97j5bsDyhG2viZu9H/hF3LrDE9Z9pRYxpK59++BVCUJEBEhtLKbaKgHyzKynmbUCLgSK4yuYWde42bOABeH754FRZtbBzDoAo8Ky9KlMEOvXp3U3IiKNRUrPg6gNd99pZlcRfLFnAdPd/SMzmwqUunsxcI2ZnQXsBNYCE8J115rZbQRJBoKurbXpihWAjh2D1zVraq4nItJMpC1BALj7s8CzCWVT4t5PBiZXs+50YHo646uiU6fgVQlCRARIbxdT46IEISJShRJEpQMOgDZtYPXqTEciItIgKEHE69RJLQgRkZASRLxOndSCEBEJKUHE69xZLQgRkZASRLzOnWHVqkxHISLSIChBxDv0UPjnPyF5EFsRkWZHCSJe166wdauG2xARQQmiqq7hyB8rVmQ2DhGRBkAJIp4ShIhIjBJEPCUIEZEYJYh4lQli+fKa64mINANKEPHat4e2bWHZsr3XFRFp4pQg4pnBEUfA559nOhIRkYxTgkiUm6sEISKCEkSy3FxYujTTUYiIZJwSRKLcXNiwAdaty3QkIiIZpQSR6Oijg9cFC2quJyLSxClBJMrPD14/+iizcYiIZJgSRKIjjoDWrZUgRKTZU4JI1KIFHHecEoSINHtKEFHy85UgRKTZU4KIkp8fjMekK5lEpBlTgohSeaL6ww8zG4eISAYpQUQ5/vjg9c03MxuHiEgGKUFE+Zd/Ce6HmDs305GIiGSMEkR1vvENeO012LUr05GIiGREWhOEmY02s0/MbKGZTaqh3hgzczMrCudzzWyrmc0Lp/9OZ5yRhg0Lhtx4//1637WISEPQMl0bNrMs4D5gJFAOlJhZsbvPT6jXFrgGeCthE4vcvTBd8e3VN74RvL7yCvTvn7EwREQyJZ0tiEHAQndf7O47gMeAsyPq3QbcCWxLYyz7rnv34Ia5p5/OdCQiIhmRzgRxOBD/aLbysCzGzPoD3d39zxHr9zSzd81srpmdHLUDM5toZqVmVrpq1ao6Czxm7Fj4+9/1CFIRaZbSmSAsosxjC81aAPcAP4qotwLo4e79geuBP5pZu6SNuU9z9yJ3L+rSpUsdhR3nggvAHWbNqvtti4g0cOlMEOVA97j5bkD8T/G2QB/gFTP7HBgMFJtZkbtvd/c1AO5eBiwCeqcx1mjHHAP9+sGMGUGiEBFpRtKZIEqAPDPraWatgAuB4sqF7r7B3Tu7e6675wJvAme5e6mZdQlPcmNmRwJ5wOI0xlq9H/wA5s2Dl1/OyO5FRDIlbQnC3XcCVwHPAwuAx939IzObamZn7WX1bwDvm9l7wGzgCndfm65Ya3TJJXDIIXDXXRnZvYhIppg3ka6ToqIiLy0tTc/G77gDJk+GF1+EU09Nzz5ERDLAzMrcvShqme6kTsV110HPnnDNNVBRkeloRETqhRJEKnJy4Fe/Cp4RceONmY5GRKReKEGk6swz4YorgnMRTz6Z6WhERNJOCWJf3HMPDB4MF10Ec+ZkOhoRkbRSgtgXOTnwl78EQ4F/61vw4IOZjkhEJG2UIPZVx47w6qvBaK8TJgT3SWzenOmoRETqnBJEbRx8MDz7LPzwh/C730FBATzxhO62FpEmRQmitrKz4Ze/DJ46l5MDY8bAwIHwzDN6yJCINAlKEPvr5JPhgw+C8ZpWr4azzoKjjgpurlu6NNPRiYjUmhJEXcjKgvHj4bPP4PHHITc3uPM6NxdOOAHuvhs+/FBdUCLSqGiojXRZtCgYJnzWLHjnnaDs0EPhtNNg+PAgcRx7bJBcREQypKahNpQg6sMXX8Df/rZnqny4UZs2UFQEgwZBnz6Qnx8MMd66dWbjFZFmQwmiIdm9O+iKevtteOut4HXevD1jPJnBkUcGjzvt3TsYA+rII4PX3NzghLiISB2pKUG0rO9gmr0WLYIb7Y4+OhhKHILksHBhMNZT/DRnDmxLeFR3165Bsjj88OB9165w2GF73nftGtyrYVEP9BMRSZ1aEA3Z7t3wz3/CkiV7psWL4fPPg+dkr1gBmzYlr5edDZ067Zk6dqz6Wvm+fXto2xbatQte27YNur1a6NoFkeZCLYjGqkWLoHVw2GEwdGh0na+/DhJF4gqY1VcAAAjnSURBVLR2LaxZE0yLFgVdWWvWwPbte99vmzZVk0a7dkHZgQcGU+vWe95HzceX5eRAq1bVT9nZSkgiDZQSRGN30EHBfRdHHbX3uu6wdeuexLFxY9ACqXyNf5/4unp1sG7iVBct0Kys6MSRWJaVBS1bRr/WdllinRYtgslsz/tMz5tVnaBuy9KxzdqWSYOiBNGcmAW/7Fu3hu7d93977kGLJDFpbNlSdb6iAnbsCKb49/FTdeWVy7ZvD+5Q37UreL9lC+zcGcwnvkaVJS7buXP/j1/SZ1+TWPx6DeG1vvdZUACPPUZdU4KQ2jMLupBycqBDh0xHs+92705OIu5B+e7dVd9net59T2ut8n1dlKVjm/UdY6XEsky/1ue+jjySdFCCkOarsgsnOzvTkYg0SDo7KCIikZQgREQkkhKEiIhEUoIQEZFIShAiIhJJCUJERCIpQYiISCQlCBERidRkRnM1s1XA/jwEujOwuo7CaSx0zM2Djrl5qO0xH+HuXaIWNJkEsb/MrLS6IW+bKh1z86Bjbh7ScczqYhIRkUhKECIiEkkJYo9pmQ4gA3TMzYOOuXmo82PWOQgREYmkFoSIiERSghARkUjNPkGY2Wgz+8TMFprZpEzHU1fMbLqZrTSzD+PKOprZHDP7LHztEJabmf06/AzeN7MBmYu89sysu5m9bGYLzOwjM7s2LG+yx21mOWb2tpm9Fx7zrWF5TzN7KzzmmWbWKiw/IJxfGC7PzWT8+8PMsszsXTP7czjfpI/ZzD43sw/MbJ6ZlYZlaf3bbtYJwsyygPuA/wccB1xkZsdlNqo6MwMYnVA2CXjR3fOAF8N5CI4/L5wmAr+rpxjr2k7gR+5+LDAY+EH479mUj3s7cKq79wMKgdFmNhj4BXBPeMzrgO+G9b8LrHP3o4B7wnqN1bXAgrj55nDMp7h7Ydz9Dun923b3ZjsBQ4Dn4+YnA5MzHVcdHl8u8GHc/CdA1/B9V+CT8P3vgYui6jXmCfgTMLK5HDfQGngHOIHgjtqWYXns7xx4HhgSvm8Z1rNMx16LY+0WfiGeCvwZsGZwzJ8DnRPK0vq33axbEMDhwLK4+fKwrKn6F3dfARC+HhKWN7nPIexG6A+8RRM/7rCrZR6wEpgDLALWu/vOsEr8ccWOOVy+AehUvxHXiXuBnwC7w/lONP1jduAFMyszs4lhWVr/tlvuR7BNgUWUNcfrfpvU52BmbYAngOvcfaNZ1OEFVSPKGt1xu/suoNDMDgaeAo6Nqha+NvpjNrMzgJXuXmZmwyuLI6o2mWMODXX35WZ2CDDHzD6uoW6dHHNzb0GUA93j5rsByzMUS334ysy6AoSvK8PyJvM5mFk2QXJ4xN2fDIub/HEDuPt64BWC8y8Hm1nlD8D444odc7i8PbC2fiPdb0OBs8zsc+Axgm6me2nax4y7Lw9fVxL8EBhEmv+2m3uCKAHywqsfWgEXAsUZjimdioHx4fvxBH30leX/Fl75MBjYUNlsbUwsaCr8L7DA3X8Zt6jJHreZdQlbDpjZgcBpBCduXwbGhNUSj7nysxgDvORhJ3Vj4e6T3b2bu+cS/J99yd3H0YSP2cwOMrO2le+BUcCHpPtvO9MnXjI9Ad8EPiXot70x0/HU4XE9CqwAKgh+TXyXoN/1ReCz8LVjWNcIruZaBHwAFGU6/loe80kEzej3gXnh9M2mfNxAAfBueMwfAlPC8iOBt4GFwCzggLA8J5xfGC4/MtPHsJ/HPxz4c1M/5vDY3gunjyq/q9L9t62hNkREJFJz72ISEZFqKEGIiEgkJQgREYmkBCEiIpGUIEREJJIShDR4ZrY5hTrXmVnrOtznOfEDN5rZVDM7ra62nw6pfE77sK0JZvb/62p70jgpQUhTcR3BYHUpC0fzrc45BCP8AuDuU9z9b7WMrcGLuwNZJEYJQhoNMxtuZq+Y2Wwz+9jMHgnvFL0GOAx42cxeDuuOMrM3zOwdM5sVjs9UOab+FDN7DTjfzC4zsxILnqfwhJm1NrMTgbOAu8Kx93uZ2QwzGxNuY0T4HIIPLHjuxgFx27413OcHZnZMxDFMMLMnzeyv4Rj+d8Yt2xz3foyZzQjfzzCz31nwrIvFZjYs3O+Cyjpx6/1XuP8XzaxLWNYr3F+Zmf29Mq5wu78MP7Nqh8A2s2+Fn2Xnff9Xk8ZMCUIam/4ErYXjCO4uHeruvyYYZ+YUdz8l/CL7D+A0dx8AlALXx21jm7uf5O6PAU+6+0APnqewAPiuu79OMFTBDR6Mvb+ockUzyyF41sZYd+9LMODllXHbXh3u83fAj6s5hkJgLNAXGGtm3aupF68DwZhDPwSeIXiuQT7Q18wKwzoHAe+E+58L3ByWTwOudvfjw5h+G7fd3uHn9KOonZrZvxI8Y+Cb7r46hTilCVGzUhqbt929HMCCIa5zgdcS6gwmSCD/CIZnohXwRtzymXHv+5jZ7cDBQBuCZwfU5Ghgibt/Gs4/CPyAYLA4gMoBAsuAc6vZxovuviE8hvnAEVQdmjnKM+7uZvYB8JW7fxCu/xHBZzCPYOjrymN7GHgybDmdCMyyPaPaHhC33VkejAYb5RSgCBjl7hv3Ep80QUoQ0thsj3u/i+i/YQPmuPtF1Wzj67j3M4Bz3P09M5tAMLZPTaodOzwhvupii6+TWC9+3JucatbZnbD+7hr24wS9BOvdvbCaOl9XUw6wmKCV1pugFSbNjLqYpKnYBLQN378JDDWzowDC8wq9q1mvLbDCgmHCx1WzvXgfA7mV2wYuIejOqQtfmdmxZtYC+NdarN+CPaOZXgy8Fv7yX2Jm50PsWcX9UtzeUoJW0ENmll+LeKSRU4KQpmIa8JyZvezuq4AJwKNm9j5Bwkg6YRy6ieCpc3MIvvwrPQbcEJ6M7lVZ6O7bgO8QdNl8QPAL/r/r6BgmETw+8yWCkXj31ddAvpmVEZyvmBqWjwO+a2aVI4GeneoG3f2TcP1Z8Z+DNA8azVVERCKpBSEiIpGUIEREJJIShIiIRFKCEBGRSEoQIiISSQlCREQiKUGIiEik/wPx/JegpdIBhAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#-----------------------------#\n",
    "#  Your code goes here        #\n",
    "#-----------------------------#\n",
    "xplot = [*range(n_iter)]\n",
    "yplot = objs\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xplot, yplot, 'r-', label='Objective value at each iteration')\n",
    "ax.set_xlabel('Interation number k')\n",
    "ax.set_ylabel('Objective Value')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. (20 pts) Using the function `predict()` and estimates in `sols`, compute training and test errors (5 pts). \n",
    "- use 'red'solid line and 'blue' dotted line for training and testing, respectively.\n",
    "- set the figsize=(10, 6.5)\n",
    "- For your convenience, the code for computing the accuracies of solutions on the solution path is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b95bf72808>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x468 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_accuracy(testX, testY, sols):\n",
    "    \"\"\"\n",
    "    returns the accuracy of each solution on the given dataset\n",
    "    \n",
    "    accuracy = percentage of correctly classified examples\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    testX, testY: input dataset\n",
    "    sols: a list of parameter vectors\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    acc: a list of accuracies, each element corresponds to the accuracy of each solution in sols\n",
    "    \"\"\"\n",
    "    acc = []\n",
    "    \n",
    "    n_examples = len(testY)\n",
    "    \n",
    "    for sol in sols:  \n",
    "        pred = predict(testX, sol)\n",
    "        n_correct = n_examples - np.count_nonzero(testY - pred)\n",
    "        acc.append((n_correct*100.0)/n_examples)\n",
    "        \n",
    "    return acc\n",
    "\n",
    "train_accs = compute_accuracy(X_train, y_train, sols)\n",
    "test_accs = compute_accuracy(X_test, y_test, sols)\n",
    "\n",
    "# plot the training/test errors against iterations\n",
    "# you need to have two series (or lines) in a single plot\n",
    "\n",
    "\n",
    "#-----------------------------#\n",
    "#  Your code goes here        #\n",
    "#-----------------------------#\n",
    "fig, ax = plt.subplots(figsize=(10, 6.5))\n",
    "ax.plot(xplot, train_accs,'r-',label='Training Accuracy')\n",
    "ax.plot(xplot, test_accs,'b.',label='Testing Accuracy')\n",
    "ax.set_xlabel('Interation number k')\n",
    "ax.set_ylabel('Accuracy in %')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
